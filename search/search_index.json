{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"BRA-LIN-S4 In-kind contribution program BRA-LIN-S4 - Photometric Redshifts. Introduction This page describes the in-kind contributions offered by the Laborat\u00f3rio Interinstitucional de e-Astronomia ( LIneA ) to Vera C. Rubin Observatory , approved as part of the in-kind contribution program BRA-LIN. This is a live document that started with the description of planned work. It is regularly updated to offer a high-level description of the software produced as the program evolves. Technical documentation of each piece of software should be delivered together with the code in the respective repositories. For comments or suggestions, please open an issue here . Section 4 of the BRA-LIN proposal refers to the contributions related to Photometric Redshifts. It is organized into four subsections (click for more details): S4.1 - PZ Training Set Maker S4.2 - PZ Server S4.3 - PZ Validation Cooperative S4.4 - PZ Tables as Federated Datasets Related documents BRA-LIN in-kind contribution proposal BRA-LIN S4 work plan approved by Rubin staff BRA-LIN S4 annual evaluation report FY2023 BRA-LIN S4 annual evaluation report FY2024 (soon) Project Management on GitHub S4.1 and S4.2 - Training Set Maker and PZ S4.3 - PZ Validation Cooperative (start in 2024) S4.4 - PZ Compute Timeline BRA-LIN S4 Timeline revised on April 2025. LIneA Key Personnel Proposal lead: Luiz Nicolaci da Costa Program manager: Julia Gschwend Contribution lead of BRA-LIN-S4: Julia Gschwend Front-end developers: Glauber Costa Vila-Verde, Jandson Vitorino Back-end developers: Cristiano Singulani, Henrique Dante, Heloisa Mengisztki System analyst: Carlos Adean Science team: Julia Gschwend, Luigi Silva, Andreia Dourado Rubin Observatory Key Personnel Primary recipient group: Rubin Photo-z Coordination Group (Contact point: Melissa Graham) In-kind Program Coordinators (IPCs) for Software Development and Science Collaboration interactions in the Rubin Operations Director\u2019s Office: Agn\u00e8s Fert\u00e9, Aprajita Verma, Greg Madejski Last updated: June 3, 2025","title":"Home"},{"location":"#bra-lin-s4","text":"In-kind contribution program BRA-LIN-S4 - Photometric Redshifts.","title":"BRA-LIN-S4"},{"location":"#introduction","text":"This page describes the in-kind contributions offered by the Laborat\u00f3rio Interinstitucional de e-Astronomia ( LIneA ) to Vera C. Rubin Observatory , approved as part of the in-kind contribution program BRA-LIN. This is a live document that started with the description of planned work. It is regularly updated to offer a high-level description of the software produced as the program evolves. Technical documentation of each piece of software should be delivered together with the code in the respective repositories. For comments or suggestions, please open an issue here . Section 4 of the BRA-LIN proposal refers to the contributions related to Photometric Redshifts. It is organized into four subsections (click for more details): S4.1 - PZ Training Set Maker S4.2 - PZ Server S4.3 - PZ Validation Cooperative S4.4 - PZ Tables as Federated Datasets","title":"Introduction"},{"location":"#related-documents","text":"BRA-LIN in-kind contribution proposal BRA-LIN S4 work plan approved by Rubin staff BRA-LIN S4 annual evaluation report FY2023 BRA-LIN S4 annual evaluation report FY2024 (soon)","title":"Related documents"},{"location":"#project-management-on-github","text":"S4.1 and S4.2 - Training Set Maker and PZ S4.3 - PZ Validation Cooperative (start in 2024) S4.4 - PZ Compute","title":"Project Management on GitHub"},{"location":"#timeline","text":"BRA-LIN S4 Timeline revised on April 2025.","title":"Timeline"},{"location":"#linea-key-personnel","text":"Proposal lead: Luiz Nicolaci da Costa Program manager: Julia Gschwend Contribution lead of BRA-LIN-S4: Julia Gschwend Front-end developers: Glauber Costa Vila-Verde, Jandson Vitorino Back-end developers: Cristiano Singulani, Henrique Dante, Heloisa Mengisztki System analyst: Carlos Adean Science team: Julia Gschwend, Luigi Silva, Andreia Dourado","title":"LIneA Key Personnel"},{"location":"#rubin-observatory-key-personnel","text":"Primary recipient group: Rubin Photo-z Coordination Group (Contact point: Melissa Graham) In-kind Program Coordinators (IPCs) for Software Development and Science Collaboration interactions in the Rubin Operations Director\u2019s Office: Agn\u00e8s Fert\u00e9, Aprajita Verma, Greg Madejski Last updated: June 3, 2025","title":"Rubin Observatory Key Personnel"},{"location":"s4_1/","text":"S4.1 - PZ Training Set Maker Introduction The Training Set Maker (TSM) is a cross-matching service tailored to create training sets to feed machine-learning-based photo-z algorithms at the catalog level. Although initially conceived as a separate service, the TSM was incorporated as a microservice under the PZ Server umbrella during early development. TSM was inspired by its namesake Training Set Maker pipeline, a Postgres-based service available on the DES Science Portal ( Gschwend et al., 2018 ), but to scale for LSST datasets. It will rely on an entirely different infrastructure: the LSDB software, currently under development by the LINCC team. The service will be delivered as a Python library with useful functions to: access, manipulate, and visualize lightweight catalogs of spectroscopic redshifts (spec-z) or true redshifts in case of simulated data locally in the user's environment (e.g., RSP notebook aspect, users' laptop, etc.). TSM will take advantage of the PZ Server's API pzserver (see API's documentation here ) to access user-generated catalogs hosted by the server. Combine different datasets and cross-match with the LSST objects catalog via an asynchronous online service using the Brazilian IDAC's computing resources. In both cases, RSP credentials are required for authentication and authorization purposes. TSM is not considered an efficient tool for manipulating big data in the user's work environment. Nonetheless, it will take advantage of the administrative functions of the PZ Server to retrieve data and metadata and use the Brazilian IDAC's infrastructure to deal with large datasets. Overview of planned remote service * The technology used to perform the cross-matching will be determined in agreement with the collaboration, following along the results of LINCC Frameworks task force on this subject. Current efforts point to the adoption of catalog files using a partitioning schema based on HIPSCat and the matching done in massive parallel processes orchestrated by LSDB , both under development. Forecasted use cases The user retrieves specific spec-z catalogs from the PZ Server or other online services such as Astroquery ( Ginsburg, Sip\u0151cz, Brasseur, et al. 2019 ). The PZ Server will provide a list of previous public spec-z catalogs of interest (previously informed by the community), for which metadata and quality flags conversion will already be available. The user combines different spec-z catalogs into a single table. TSM will provide a mechanism to perform spatial cross-matching among multiple tables to resolve multiple spec-z measurements for the same galaxies (flexible criteria) and standardize quality flags from different surveys to create spec-z compilations with redshifts from different sources. The user retrieves spec-z compilations from the PZ Server. The PZ Server will host a standardized compilation of spec-z with public data available up to date on each LSST Data Release, vetted by LSST DM staff. These compilations must be easy to find (e.g., flagged as \"latest\") and contain detailed documentation to cite the original sources properly. The user combines a spec-z table with photometric data from the LSST Objects Catalog to build a training set for photo-z codes. TSM will provide a mechanism to submit jobs to LIneA's HPC cluster to perform spatial cross-matching between a spec-z table registered on the PZ Server and the copy of the LSST Object Catalog hosted in the Brazilian IDAC. The matching results will be registered on the PZ Server as a new data product. Metadata and data access instructions will be sent back to the user. The user splits the matched spec-photo catalog into two or more subsamples (for training and validation/test purposes). TSM will provide a simple method to split the data randomly into a finite number of parts, with proportions defined by the user. Additional methods that follow specific science-driven criteria to define subsamples or apply some transformation of the data (e.g., weighting or data augmentation) are not part of the scope of the in-kind contribution but can be added to the library later. Contributions from the community are very welcome. The LSST PZ Commissioning Team uses the TSM to create standardized training and validation sets for the Photo-z Validation Cooperative and distribute them to the community. The resulting catalogs will be formatted according to the LSST requirements described in the DMTN-049 - A Roadmap to Photometric Redshifts for the LSST Object Catalog (to be defined by LSST Data Management (DM) System Science Team) and contain all the provenance information necessary to be reproduced using the same TSM tools by any user.","title":"BRA-LIN-S4.1"},{"location":"s4_1/#s41-pz-training-set-maker","text":"","title":"S4.1 - PZ Training Set Maker"},{"location":"s4_1/#introduction","text":"The Training Set Maker (TSM) is a cross-matching service tailored to create training sets to feed machine-learning-based photo-z algorithms at the catalog level. Although initially conceived as a separate service, the TSM was incorporated as a microservice under the PZ Server umbrella during early development. TSM was inspired by its namesake Training Set Maker pipeline, a Postgres-based service available on the DES Science Portal ( Gschwend et al., 2018 ), but to scale for LSST datasets. It will rely on an entirely different infrastructure: the LSDB software, currently under development by the LINCC team. The service will be delivered as a Python library with useful functions to: access, manipulate, and visualize lightweight catalogs of spectroscopic redshifts (spec-z) or true redshifts in case of simulated data locally in the user's environment (e.g., RSP notebook aspect, users' laptop, etc.). TSM will take advantage of the PZ Server's API pzserver (see API's documentation here ) to access user-generated catalogs hosted by the server. Combine different datasets and cross-match with the LSST objects catalog via an asynchronous online service using the Brazilian IDAC's computing resources. In both cases, RSP credentials are required for authentication and authorization purposes. TSM is not considered an efficient tool for manipulating big data in the user's work environment. Nonetheless, it will take advantage of the administrative functions of the PZ Server to retrieve data and metadata and use the Brazilian IDAC's infrastructure to deal with large datasets.","title":"Introduction"},{"location":"s4_1/#overview-of-planned-remote-service","text":"* The technology used to perform the cross-matching will be determined in agreement with the collaboration, following along the results of LINCC Frameworks task force on this subject. Current efforts point to the adoption of catalog files using a partitioning schema based on HIPSCat and the matching done in massive parallel processes orchestrated by LSDB , both under development.","title":"Overview of planned remote service"},{"location":"s4_1/#forecasted-use-cases","text":"The user retrieves specific spec-z catalogs from the PZ Server or other online services such as Astroquery ( Ginsburg, Sip\u0151cz, Brasseur, et al. 2019 ). The PZ Server will provide a list of previous public spec-z catalogs of interest (previously informed by the community), for which metadata and quality flags conversion will already be available. The user combines different spec-z catalogs into a single table. TSM will provide a mechanism to perform spatial cross-matching among multiple tables to resolve multiple spec-z measurements for the same galaxies (flexible criteria) and standardize quality flags from different surveys to create spec-z compilations with redshifts from different sources. The user retrieves spec-z compilations from the PZ Server. The PZ Server will host a standardized compilation of spec-z with public data available up to date on each LSST Data Release, vetted by LSST DM staff. These compilations must be easy to find (e.g., flagged as \"latest\") and contain detailed documentation to cite the original sources properly. The user combines a spec-z table with photometric data from the LSST Objects Catalog to build a training set for photo-z codes. TSM will provide a mechanism to submit jobs to LIneA's HPC cluster to perform spatial cross-matching between a spec-z table registered on the PZ Server and the copy of the LSST Object Catalog hosted in the Brazilian IDAC. The matching results will be registered on the PZ Server as a new data product. Metadata and data access instructions will be sent back to the user. The user splits the matched spec-photo catalog into two or more subsamples (for training and validation/test purposes). TSM will provide a simple method to split the data randomly into a finite number of parts, with proportions defined by the user. Additional methods that follow specific science-driven criteria to define subsamples or apply some transformation of the data (e.g., weighting or data augmentation) are not part of the scope of the in-kind contribution but can be added to the library later. Contributions from the community are very welcome. The LSST PZ Commissioning Team uses the TSM to create standardized training and validation sets for the Photo-z Validation Cooperative and distribute them to the community. The resulting catalogs will be formatted according to the LSST requirements described in the DMTN-049 - A Roadmap to Photometric Redshifts for the LSST Object Catalog (to be defined by LSST Data Management (DM) System Science Team) and contain all the provenance information necessary to be reproduced using the same TSM tools by any user.","title":"Forecasted use cases"},{"location":"s4_2/","text":"S4.2 - Photo-z Server Introduction Inspired by features of the DES Science Portal ( Gschwend et al., 2018 ; Fausti Neto et al., 2018 ), the Photo-z (PZ) Server is being developed to be an online service, complementary to the Rubin Science Platform (RSP), to host PZ-related lightweight data products and to offer data management tools that allow sharing data products among RSP users, attach and share relevant metadata, and help on provenance tracking. The PZ Server is hosted at the Brazilian Independent Data Access Center (IDAC) and is open to all RSP users (LSST data rights holders) without geographic constraints. It is designed to be as broad and generic as possible to be helpful to all LSST Science Collaborations working with PZ data products. As required by the LSST in-kind program, the source code will be publicly available on GitHub . The PZ Server was designed to focus on helping RSP users participate in the PZ Validation Cooperative. This DM team initiative will occur during the LSST commissioning phase (see technical note dmtn-049 for details). The PZ Coordination Group will receive \"admin\" user credentials with special permissions to add data products tagged as \"official data products\". During the PZ Validation Cooperative, the PZ Coordination Group can use the PZ Server to host and distribute standardized training and validation sets to be used in algorithm performance comparison experiments and collect the results of different users. Nonetheless, the PZ Server will continue serving the LSST Community in subsequent years. Beyond the PZ Validation Cooperative, the RSP users can use the PZ Server to easily keep track and share lightweight files containing varied test results. Photo-z Server Website The PZ Server will be accessible via the website ( pzserver.linea.org.br ), where the users can upload user-generated data products and browse to find and download data products from a list. The official data products are listed on a separate page (the card on the left side of the landing page). During the development phase, a test environment is available at pz-server-dev.linea.org.br , open to LSST members with a valid RSP account. Feedback from future PZ Server users is more than welcome. Landing Page Data Products List Page Data Product Download Page Display description file Display table preview PZ Server API The PZ Server also offers an API as a Python package to facilitate the command-line access of data and metadata. The API contains functions to explore the data products available, retrieve the contents of a given data product to work on memory or download the files of interest. The Python package pzserver is also available on GitHub and is installable via pip with: pip install pzserver Please go to the PZ Server API documentation page for details. A tutorial is available for new users as Jupyter Notebook on the pzserver repository. To access the notebook: git clone https://github.com/linea-it/pzserver and find it inside the docs/notebooks/ folder.","title":"BRA-LIN-S4.2"},{"location":"s4_2/#s42-photo-z-server","text":"","title":"S4.2 - Photo-z Server"},{"location":"s4_2/#introduction","text":"Inspired by features of the DES Science Portal ( Gschwend et al., 2018 ; Fausti Neto et al., 2018 ), the Photo-z (PZ) Server is being developed to be an online service, complementary to the Rubin Science Platform (RSP), to host PZ-related lightweight data products and to offer data management tools that allow sharing data products among RSP users, attach and share relevant metadata, and help on provenance tracking. The PZ Server is hosted at the Brazilian Independent Data Access Center (IDAC) and is open to all RSP users (LSST data rights holders) without geographic constraints. It is designed to be as broad and generic as possible to be helpful to all LSST Science Collaborations working with PZ data products. As required by the LSST in-kind program, the source code will be publicly available on GitHub . The PZ Server was designed to focus on helping RSP users participate in the PZ Validation Cooperative. This DM team initiative will occur during the LSST commissioning phase (see technical note dmtn-049 for details). The PZ Coordination Group will receive \"admin\" user credentials with special permissions to add data products tagged as \"official data products\". During the PZ Validation Cooperative, the PZ Coordination Group can use the PZ Server to host and distribute standardized training and validation sets to be used in algorithm performance comparison experiments and collect the results of different users. Nonetheless, the PZ Server will continue serving the LSST Community in subsequent years. Beyond the PZ Validation Cooperative, the RSP users can use the PZ Server to easily keep track and share lightweight files containing varied test results.","title":"Introduction"},{"location":"s4_2/#photo-z-server-website","text":"The PZ Server will be accessible via the website ( pzserver.linea.org.br ), where the users can upload user-generated data products and browse to find and download data products from a list. The official data products are listed on a separate page (the card on the left side of the landing page). During the development phase, a test environment is available at pz-server-dev.linea.org.br , open to LSST members with a valid RSP account. Feedback from future PZ Server users is more than welcome.","title":"Photo-z Server Website"},{"location":"s4_2/#landing-page","text":"","title":"Landing Page"},{"location":"s4_2/#data-products-list-page","text":"","title":"Data Products List Page"},{"location":"s4_2/#data-product-download-page","text":"Display description file Display table preview","title":"Data Product Download Page"},{"location":"s4_2/#pz-server-api","text":"The PZ Server also offers an API as a Python package to facilitate the command-line access of data and metadata. The API contains functions to explore the data products available, retrieve the contents of a given data product to work on memory or download the files of interest. The Python package pzserver is also available on GitHub and is installable via pip with: pip install pzserver Please go to the PZ Server API documentation page for details. A tutorial is available for new users as Jupyter Notebook on the pzserver repository. To access the notebook: git clone https://github.com/linea-it/pzserver and find it inside the docs/notebooks/ folder.","title":"PZ Server API"},{"location":"s4_3/","text":"S4.3 - PZ Validation Cooperative The third contribution refers to offering help on the PZ Validation Cooperative by LIneA staff with expertise in photo-zs. This contribution is offered in terms of FTEs to execute tasks defined by the recipient group at the Cooperative (commissioning phase) epoch. To know more about the PZ Validation Cooperative organized by LSST DM, please access the document DMTN-049 - A Roadmap to Photometric Redshifts for the LSST Object Catalog .","title":"BRA-LIN-S4.3"},{"location":"s4_3/#s43-pz-validation-cooperative","text":"The third contribution refers to offering help on the PZ Validation Cooperative by LIneA staff with expertise in photo-zs. This contribution is offered in terms of FTEs to execute tasks defined by the recipient group at the Cooperative (commissioning phase) epoch. To know more about the PZ Validation Cooperative organized by LSST DM, please access the document DMTN-049 - A Roadmap to Photometric Redshifts for the LSST Object Catalog .","title":"S4.3 - PZ Validation Cooperative"},{"location":"s4_4/","text":"S4.4 - PZ Tables as Federated Datasets Introduction The Legacy Survey of Space and Time (LSST) will provide photometric measurements for billions of objects during its ten years of operations. Most foreseen LSST science cases rely on photometric redshifts (photo-z) estimates for these objects. The LSST Project's Data Management (DM) department plans to provide at least one, possibly more, photo-z estimates for each object as part of each data release. Given the large and diverse scope of science that can result from the LSST data, a unique photo-z method is expected not to satisfy all the requirements of the whole community. This contribution consists of offering photo-z tables as federated datasets for each data release, using a different photo-z method from the official estimates (also to be defined by the DM team), thus expanding the scope of the science supported by the data releases. The infrastructure required to produce, store, and deliver the photo-z tables will be provided by the Brazilian IDAC. The software development necessary to produce these tables, including the optimization and refactoring of the DES photo-z pipelines to run on the LSST scale and the production of new pipelines to cover all data flow steps, is accounted for as a directable software development effort. Software development The initial software development plan consisted of refactoring the pipeline Photo-z Compute from the DES Science Portal to ensure scalability in LSST. The new pipeline would keep only the concepts of data preparation, parallelization, easy access to metadata, and provenance control. The LSST scale imposes the adoption of a completely different technology from that used in the DES Portal. Preliminary tests using Parsl to handle the parallelization and using the photo-z code LePHARE were carried out in 2021-2022. In mid-2022, the development team started an investigation to evaluate the possibility of reusing RAIL (open source code, developed by DESC) infrastructure to support the production of photo-z tables. An initial set of scripts to automatize the preparation of input data and the execution of the RAIL estimate module at LIneA's HPC environment is available on the PZ Compute's repository on GitHub . It started supporting two photo-z algorithms, BPZ and FlexZBoost, then was expanded to include GPz, TPZ, and LePHARE. Scalability tests on DP0.2 data yielded the first runtime and storage benchmarks. Detailed reports with test results are available in these two documents: report_pz_compute_henrique_dante_may_2023.pdf report_pz_compute_henrique_dante_dec_2023.pdf In summary, the main results are: For 40 billion objects, around 267 thousand files are expected. Uncompressed output files with 301 points would take around 344 MiB. The result of estimating 40 billion objects would take around 88 TiB. 12 column pre-processed input files with magnitude data would take around 14 MiB each, 3,5 TiB for DR11 (40 billion objects). Algorithm Predicted Runtime for DR11 Pr\u00e9-processing 24 min FlexZBost 7 hours BPZ 9 hours TPZ 9 hours LePHARE 24 days Roadmap for the development phase Development of scripts to automatize data integrity tests after the data transfer. Development of scripts to automatize the creation of skinny tables (lightweight input tables). Development of Jupyter notebooks to perform PZ training and validation. Development of scripts to automatize the execution of RAIL with parallelization in HPC Cluster Development of pipeline PZ Compute's code profiler and process monitoring Scalability tests using the IDAC's infrastructure (Apolo computer cluster and the supercomputer Santos Dumont) Integration tests - E2E pre-operations using data previews DP0.2, DP1, DP2 Wrapping new photo-z codes into RAIL (if necessary). Performance tests - comparing codes (using DP1 and DP2). Planning for operations The figure below shows a flowchart representing the data flow within the Brazilian IDAC infrastructure. The numbered arrows refer to the sequence of processes that involves moving data through the IDAC components. The roman numbers refer to processes involving data transformation. The steps involved in the production of PZ tables are organized in four stages: Stage I - Data acquisition Register new data release (DR) on the IDAC data management system (initial metadata). Start data transfer: download LSST Objects Catalog files from LSST Data Access Center (DAC) to LIneA's Data Transfer Node in DMZ. Save Objects Catalog files on Lustre T1. Objects Catalog validation - integrity and consistency checks, QA reports, and compare key numbers. Append QA info to DR metadata. Apply data cleaning to create a \"skinny table\" for input for photo-z pipelines (e.g., select columns, apply photometric corrections, round extra decimal cases, etc). Save skinny table on Lustre T0 (fast I/O). Append skinny table info to DR metadata. Download and store ancillary files (e.g., observing conditions maps, SED templates, documents, etc). Append ancillary file info to DR metadata. Stage II - Photo-z pre-processing Execution of pipeline Training Set Maker (x-matching). Characterization of the training set - QA report (Jupyter Notebook). Register (and upload) training set(s) on the PZ Server. Photo-z Validation: execution of RAIL inform , estimation , and evaluation modules. Report with photo-z quality metrics (Jupyter Notebook). Obtain report approval from Rubin DM Photo-z Coord. Register (and upload) PZ validation results on the PZ Server. Stage III - Photo-z computing Execution of pipeline PZ Compute on the whole dataset. Validation of PZ Compute results - Report with metadata, priors, configuration parameters, global statistics, N(z) (global and tomographic bins) (Jupyter Notebook). Register (no upload, only metadata) the PZ table on the PZ Server. Stage IV - Photo-z post-processing Data preparation for upload (compress files, prepare a package with reports and metadata). Move the PZ table to LIneA's Data Transfer Node in DMZ. Deliver PZ tables (data transfer from BR IDAC to US DAC or Cloud). Register on RSP as a federated dataset. Make the table stored in USDF available for RSP users.","title":"BRA-LIN-S4.4"},{"location":"s4_4/#s44-pz-tables-as-federated-datasets","text":"","title":"S4.4 - PZ Tables as Federated Datasets"},{"location":"s4_4/#introduction","text":"The Legacy Survey of Space and Time (LSST) will provide photometric measurements for billions of objects during its ten years of operations. Most foreseen LSST science cases rely on photometric redshifts (photo-z) estimates for these objects. The LSST Project's Data Management (DM) department plans to provide at least one, possibly more, photo-z estimates for each object as part of each data release. Given the large and diverse scope of science that can result from the LSST data, a unique photo-z method is expected not to satisfy all the requirements of the whole community. This contribution consists of offering photo-z tables as federated datasets for each data release, using a different photo-z method from the official estimates (also to be defined by the DM team), thus expanding the scope of the science supported by the data releases. The infrastructure required to produce, store, and deliver the photo-z tables will be provided by the Brazilian IDAC. The software development necessary to produce these tables, including the optimization and refactoring of the DES photo-z pipelines to run on the LSST scale and the production of new pipelines to cover all data flow steps, is accounted for as a directable software development effort.","title":"Introduction"},{"location":"s4_4/#software-development","text":"The initial software development plan consisted of refactoring the pipeline Photo-z Compute from the DES Science Portal to ensure scalability in LSST. The new pipeline would keep only the concepts of data preparation, parallelization, easy access to metadata, and provenance control. The LSST scale imposes the adoption of a completely different technology from that used in the DES Portal. Preliminary tests using Parsl to handle the parallelization and using the photo-z code LePHARE were carried out in 2021-2022. In mid-2022, the development team started an investigation to evaluate the possibility of reusing RAIL (open source code, developed by DESC) infrastructure to support the production of photo-z tables. An initial set of scripts to automatize the preparation of input data and the execution of the RAIL estimate module at LIneA's HPC environment is available on the PZ Compute's repository on GitHub . It started supporting two photo-z algorithms, BPZ and FlexZBoost, then was expanded to include GPz, TPZ, and LePHARE. Scalability tests on DP0.2 data yielded the first runtime and storage benchmarks. Detailed reports with test results are available in these two documents: report_pz_compute_henrique_dante_may_2023.pdf report_pz_compute_henrique_dante_dec_2023.pdf In summary, the main results are: For 40 billion objects, around 267 thousand files are expected. Uncompressed output files with 301 points would take around 344 MiB. The result of estimating 40 billion objects would take around 88 TiB. 12 column pre-processed input files with magnitude data would take around 14 MiB each, 3,5 TiB for DR11 (40 billion objects). Algorithm Predicted Runtime for DR11 Pr\u00e9-processing 24 min FlexZBost 7 hours BPZ 9 hours TPZ 9 hours LePHARE 24 days","title":"Software development"},{"location":"s4_4/#roadmap-for-the-development-phase","text":"Development of scripts to automatize data integrity tests after the data transfer. Development of scripts to automatize the creation of skinny tables (lightweight input tables). Development of Jupyter notebooks to perform PZ training and validation. Development of scripts to automatize the execution of RAIL with parallelization in HPC Cluster Development of pipeline PZ Compute's code profiler and process monitoring Scalability tests using the IDAC's infrastructure (Apolo computer cluster and the supercomputer Santos Dumont) Integration tests - E2E pre-operations using data previews DP0.2, DP1, DP2 Wrapping new photo-z codes into RAIL (if necessary). Performance tests - comparing codes (using DP1 and DP2).","title":"Roadmap for the development phase"},{"location":"s4_4/#planning-for-operations","text":"The figure below shows a flowchart representing the data flow within the Brazilian IDAC infrastructure. The numbered arrows refer to the sequence of processes that involves moving data through the IDAC components. The roman numbers refer to processes involving data transformation. The steps involved in the production of PZ tables are organized in four stages:","title":"Planning for operations"},{"location":"s4_4/#stage-i-data-acquisition","text":"Register new data release (DR) on the IDAC data management system (initial metadata). Start data transfer: download LSST Objects Catalog files from LSST Data Access Center (DAC) to LIneA's Data Transfer Node in DMZ. Save Objects Catalog files on Lustre T1. Objects Catalog validation - integrity and consistency checks, QA reports, and compare key numbers. Append QA info to DR metadata. Apply data cleaning to create a \"skinny table\" for input for photo-z pipelines (e.g., select columns, apply photometric corrections, round extra decimal cases, etc). Save skinny table on Lustre T0 (fast I/O). Append skinny table info to DR metadata. Download and store ancillary files (e.g., observing conditions maps, SED templates, documents, etc). Append ancillary file info to DR metadata.","title":"Stage I   - Data acquisition"},{"location":"s4_4/#stage-ii-photo-z-pre-processing","text":"Execution of pipeline Training Set Maker (x-matching). Characterization of the training set - QA report (Jupyter Notebook). Register (and upload) training set(s) on the PZ Server. Photo-z Validation: execution of RAIL inform , estimation , and evaluation modules. Report with photo-z quality metrics (Jupyter Notebook). Obtain report approval from Rubin DM Photo-z Coord. Register (and upload) PZ validation results on the PZ Server.","title":"Stage II  - Photo-z pre-processing"},{"location":"s4_4/#stage-iii-photo-z-computing","text":"Execution of pipeline PZ Compute on the whole dataset. Validation of PZ Compute results - Report with metadata, priors, configuration parameters, global statistics, N(z) (global and tomographic bins) (Jupyter Notebook). Register (no upload, only metadata) the PZ table on the PZ Server.","title":"Stage III - Photo-z computing"},{"location":"s4_4/#stage-iv-photo-z-post-processing","text":"Data preparation for upload (compress files, prepare a package with reports and metadata). Move the PZ table to LIneA's Data Transfer Node in DMZ. Deliver PZ tables (data transfer from BR IDAC to US DAC or Cloud). Register on RSP as a federated dataset. Make the table stored in USDF available for RSP users.","title":"Stage IV - Photo-z post-processing"}]}