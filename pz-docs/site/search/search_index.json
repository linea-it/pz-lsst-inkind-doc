{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"BRA-LIN-S4 In-kind contribution program BRA-LIN-S4 - Photometric Redshifts. Introduction This page describes the in-kind contributions offered by the Laborat\u00f3rio Interinstitucional de e-Astronomia ( LIneA ) to Vera C. Rubin Observatory 's PZ Coordination Group, approved as part of the in-kind contribution program BRA-LIN. The BRA-LIN in-kind contribution proposal document is available on this link . This is a live document started with the description of planned work, to be constantly updated to offer a high-level description of the software produced, as long as the program evolves. Technical documentation of each piece of software should be delivered together with the code in the respective repositories. The Section 4 of the BRA-LIN proposal refers to the contributions related to Photometric Redshifts. It is organized in four subsections (click for more details): S4.1 - PZ Training Set Maker S4.2 - PZ Server S4.3 - PZ Validation Cooperative S4.4 - PZ Server LIneA Key Personnel Proposal Lead: Luiz Nicolaci da Costa Program Manager: Julia Gschwend Contribution Lead of BRA-LIN-S4: Julia Gschwend ( contact ) Software developers: Cristiano Singulani, Glauber Costa Vila-Verde System Analyst: Carlos Adean Rubin Observatory Key Personnel Recipient Group: Rubin Construction Data Management Subsystem - Rubin Photo-z Coordination Group. Contact point: Melissa Graham In-kind Program Coordinators (IPCs) for \u200b\u200bSoftware Development and Science Collaboration interactions in the Rubin Operations Director\u2019s Office: Aprajita Verma, Greg Madejski","title":"Home"},{"location":"#bra-lin-s4","text":"In-kind contribution program BRA-LIN-S4 - Photometric Redshifts.","title":"BRA-LIN-S4"},{"location":"#introduction","text":"This page describes the in-kind contributions offered by the Laborat\u00f3rio Interinstitucional de e-Astronomia ( LIneA ) to Vera C. Rubin Observatory 's PZ Coordination Group, approved as part of the in-kind contribution program BRA-LIN. The BRA-LIN in-kind contribution proposal document is available on this link . This is a live document started with the description of planned work, to be constantly updated to offer a high-level description of the software produced, as long as the program evolves. Technical documentation of each piece of software should be delivered together with the code in the respective repositories. The Section 4 of the BRA-LIN proposal refers to the contributions related to Photometric Redshifts. It is organized in four subsections (click for more details): S4.1 - PZ Training Set Maker S4.2 - PZ Server S4.3 - PZ Validation Cooperative S4.4 - PZ Server","title":"Introduction"},{"location":"#linea-key-personnel","text":"Proposal Lead: Luiz Nicolaci da Costa Program Manager: Julia Gschwend Contribution Lead of BRA-LIN-S4: Julia Gschwend ( contact ) Software developers: Cristiano Singulani, Glauber Costa Vila-Verde System Analyst: Carlos Adean","title":"LIneA Key Personnel"},{"location":"#rubin-observatory-key-personnel","text":"Recipient Group: Rubin Construction Data Management Subsystem - Rubin Photo-z Coordination Group. Contact point: Melissa Graham In-kind Program Coordinators (IPCs) for \u200b\u200bSoftware Development and Science Collaboration interactions in the Rubin Operations Director\u2019s Office: Aprajita Verma, Greg Madejski","title":"Rubin Observatory Key Personnel"},{"location":"s4_1/","text":"S4.1 - PZ Training Set Maker The Python package pz_tsm (Training Set Maker for Photo-zs) is inspired by its namesake Training Set Maker pipeline, available on the DES Science Portal ( Gschwend et al., 2018 ), but will rely on completely different infrastructure. The new Training Set Maker will take advantage of the administrative functions of the PZ Server to retrieve data and metadata, and the Brazilian IDAC infrastructure, to perform combinations of different datasets and crossmatch with the LSST objects catalog. The package is designed to handle lightweight catalogs of spectroscopic redshifts (spec-z) to be used for the purpose of training and validation of PZ algorithms. It will offer useful Python functions to manipulate and visualize these catalogs that are expected to contain the order of hundreds of thousands of objects. It is by no means to be considered a tool to manipulate big data. The figure below shows the data flow through a series of steps. In the very beginning, it will use available tools such as Astroquery ( Ginsburg, Sip\u0151cz, Brasseur, et al 2019 ) to retrieve public spec-z catalogs available online. Alternatively, it would take advantage of the PZ Server API aspect to access user-generated catalogs hosted by the server. The package will provide a function to submit cross-matching jobs, using the PZ Server infrastructure, to combine a given spec-z catalog with the LSST objects catalog and return the photometric data of interest for the construction of training and validation sets (e.g. magnitudes and respective errors). The further manipulation of the matched data to define the training and validation subsamples might be strongly dependent on the specific science case, therefore out of the scope of this contribution. It is expected that the pz_tsm package will receive contributions from the community for the implementation of science-driven methods of subsample definition (e.g. with weights or data augmentation). The core package will provide a simple random split of the matched sample into training and validation subsamples as default. Finally, the resulting training and validation sets created will be formatted according to the LSST requirements described in the DMTN-049 - A Roadmap to Photometric Redshifts for the LSST Object Catalog (to be defined by LSST Data Management (DM) System Science Team) and contain all the provenance information necessary to be considered reproducible and ready to be uploaded on the PZ Server.","title":"BRA-LIN-S4.1"},{"location":"s4_1/#s41-pz-training-set-maker","text":"The Python package pz_tsm (Training Set Maker for Photo-zs) is inspired by its namesake Training Set Maker pipeline, available on the DES Science Portal ( Gschwend et al., 2018 ), but will rely on completely different infrastructure. The new Training Set Maker will take advantage of the administrative functions of the PZ Server to retrieve data and metadata, and the Brazilian IDAC infrastructure, to perform combinations of different datasets and crossmatch with the LSST objects catalog. The package is designed to handle lightweight catalogs of spectroscopic redshifts (spec-z) to be used for the purpose of training and validation of PZ algorithms. It will offer useful Python functions to manipulate and visualize these catalogs that are expected to contain the order of hundreds of thousands of objects. It is by no means to be considered a tool to manipulate big data. The figure below shows the data flow through a series of steps. In the very beginning, it will use available tools such as Astroquery ( Ginsburg, Sip\u0151cz, Brasseur, et al 2019 ) to retrieve public spec-z catalogs available online. Alternatively, it would take advantage of the PZ Server API aspect to access user-generated catalogs hosted by the server. The package will provide a function to submit cross-matching jobs, using the PZ Server infrastructure, to combine a given spec-z catalog with the LSST objects catalog and return the photometric data of interest for the construction of training and validation sets (e.g. magnitudes and respective errors). The further manipulation of the matched data to define the training and validation subsamples might be strongly dependent on the specific science case, therefore out of the scope of this contribution. It is expected that the pz_tsm package will receive contributions from the community for the implementation of science-driven methods of subsample definition (e.g. with weights or data augmentation). The core package will provide a simple random split of the matched sample into training and validation subsamples as default. Finally, the resulting training and validation sets created will be formatted according to the LSST requirements described in the DMTN-049 - A Roadmap to Photometric Redshifts for the LSST Object Catalog (to be defined by LSST Data Management (DM) System Science Team) and contain all the provenance information necessary to be considered reproducible and ready to be uploaded on the PZ Server.","title":"S4.1 - PZ Training Set Maker"},{"location":"s4_2/","text":"S4.2 - PZ Server Inspired by features of the DES Science Portal ( Gschwend et al., 2018 ; Fausti Neto et al., 2018 ), the PZ Server is being planned to be an online service, complementary to the Rubin Science Platform (RSP), to host PZ-related lightweight data products and to offer data management tools that allow sharing data products among RSP users, attach and share relevant metadata, and help on provenance tracking. The PZ Server will be hosted in the Brazilian Independent Data Access Center (IDAC), being accessible via an independent URL (https://pz-server.linea.org.br), open to all RSP users (LSST data rights holders), without geographic constraints. It is planned to be as broad and generic as possible to be useful to all LSST Science Collaborations working with PZ data products. As required by LSST in-kind program, the source code will be publicly available on GitHub (https://github.com/linea-it/pz-server). The PZ Coordination Group will receive \"admin\" user credentials with special permissions. It will also be able to request uploads of official data products on-demand to the LIneA team. The PZ Server is being designed with a special focus on helping RSP users participating in the PZ Validation Cooperative, a DM team's initiative that will take place during LSST commissioning phase (see technical note dmtn-049 for details), but it is planned to continue serving the LSST Community during subsequent years. During the PZ Validation Cooperative, the PZ Coordination Group will be able to use the PZ Server to host and distribute standardized training and validation sets to be used in algorithm performance comparison experiments and to collect the results obtained by different users. Beyond the PZ Validation Cooperative, the RSP users can use the PZ Server to easily keep track and share lightweight files containing varied test results. However, all data products uploaded to the PZ Server will automatically be visible and available, without any scientific validation, to all RSP users and only for this particular group. Therefore it is not the appropriate tool to release data products to the general public.","title":"BRA-LIN-S4.2"},{"location":"s4_2/#s42-pz-server","text":"Inspired by features of the DES Science Portal ( Gschwend et al., 2018 ; Fausti Neto et al., 2018 ), the PZ Server is being planned to be an online service, complementary to the Rubin Science Platform (RSP), to host PZ-related lightweight data products and to offer data management tools that allow sharing data products among RSP users, attach and share relevant metadata, and help on provenance tracking. The PZ Server will be hosted in the Brazilian Independent Data Access Center (IDAC), being accessible via an independent URL (https://pz-server.linea.org.br), open to all RSP users (LSST data rights holders), without geographic constraints. It is planned to be as broad and generic as possible to be useful to all LSST Science Collaborations working with PZ data products. As required by LSST in-kind program, the source code will be publicly available on GitHub (https://github.com/linea-it/pz-server). The PZ Coordination Group will receive \"admin\" user credentials with special permissions. It will also be able to request uploads of official data products on-demand to the LIneA team. The PZ Server is being designed with a special focus on helping RSP users participating in the PZ Validation Cooperative, a DM team's initiative that will take place during LSST commissioning phase (see technical note dmtn-049 for details), but it is planned to continue serving the LSST Community during subsequent years. During the PZ Validation Cooperative, the PZ Coordination Group will be able to use the PZ Server to host and distribute standardized training and validation sets to be used in algorithm performance comparison experiments and to collect the results obtained by different users. Beyond the PZ Validation Cooperative, the RSP users can use the PZ Server to easily keep track and share lightweight files containing varied test results. However, all data products uploaded to the PZ Server will automatically be visible and available, without any scientific validation, to all RSP users and only for this particular group. Therefore it is not the appropriate tool to release data products to the general public.","title":"S4.2 - PZ Server"},{"location":"s4_3/","text":"S4.3 - PZ Validation Cooperative The third contribution refers to offering help on the PZ Validation Cooperative by LIneA staff with expertise in photo-zs. This contribution is offered in terms of FTEs to execute tasks defined by the recipient group at the epoch of the Cooperative (commissioning phase). To know more about the PZ Validation Cooperative organized by LSST DM, please access the document DMTN-049 - A Roadmap to Photometric Redshifts for the LSST Object Catalog .","title":"BRA-LIN-S4.3"},{"location":"s4_3/#s43-pz-validation-cooperative","text":"The third contribution refers to offering help on the PZ Validation Cooperative by LIneA staff with expertise in photo-zs. This contribution is offered in terms of FTEs to execute tasks defined by the recipient group at the epoch of the Cooperative (commissioning phase). To know more about the PZ Validation Cooperative organized by LSST DM, please access the document DMTN-049 - A Roadmap to Photometric Redshifts for the LSST Object Catalog .","title":"S4.3 - PZ Validation Cooperative"},{"location":"s4_4/","text":"S4.4 - PZ Tables as Federated Datasets During its ten years of operations, the Legacy Survey of Space and Time (LSST) will provide photometric measurements for billions of objects. Most of the foreseen LSST science cases will rely on photometric redshifts (photo-z) estimates for these objects. The LSST project plan to provide at least one, possibly more, photo-z estimates for each object as part of each data release. Given the large and diverse scope of science that can result from the LSST data, a unique photo-z method is expected to not satisfy all the requirements of the whole community. As part of the BRA-LIN in-kind contribution program, this contribution consists in offering photo-z tables as federated datasets for each data release, using a different photo-z method from the official estimates (to be defined by the DM team), thus expanding the scope of the science supported by the data releases. The infrastructure required to produce, store, and deliver the photo-z tables will be provided by the Brazilian IDAC. The software development necessary to produce these tables, which include the optimization and refactoring of the DES photo-z pipelines to run on the LSST scale and the production of new pipelines to cover all steps of the data flow, is accounted for as a directable software development effort. Software development The initial plan of software development consisted of refactoring the pipeline Photo-z Compute from the DES Science Portal, to ensure scalability in LSST. The new pipeline would keep only the concepts of data preparation, parallelization, easy access to metadata, and provenance control. The LSST scale imposes the adoption of a completely different techlogoly from that used in the DES Portal. A preliminary version of the new pipeline Photo-z Compute is being developed using Parsl to handle the parallelization and using the photo-z code LePHARE as an example of an algorithm for tests. The code under development is available on Github . In mid-2022, the development team started an investigation to evaluate the possibility of reusing RAIL (open source, developed by DESC) infrastructure to support the production of photo-z tables. If the team decides to adopt RAIL, they would contribute to RAIL development by implementing the wrapper of the photo-z algorithm recommended by DM. Planning for operations The figure above shows a flowchart representing the data flow within the Brazilian IDAC infrastructure. The numbered arrows refer to the sequence of processes that involves moving data through the IDAC components. The roman numbers refer to processes involving data transformation: (1) Download LSST Objects Catalog from LSST Data Access Center (DAC) to LIneA's Data Transfer Node in DMZ. (2) Move LSST Objects Catalog to Lustre Tier 1 (3) Read data ( I ) Apply data cleaning to create a \"skinny table\" to be used as input for photo-z pipelines (e.g., select columns, truncate extra decimal cases, etc) (4) Ingest skinny table into Lustre Tier 0 (fast throughput) (5) Read data ( II ) Run PZ Compute pipeline (6) Write the resulting PZ table on Lustre Tier 1 (7) Move the PZ table to LIneA's Data Transfer Node in DMZ (8) Upload the PZ table to LSST DAC as a federated dataset","title":"BRA-LIN-S4.4"},{"location":"s4_4/#s44-pz-tables-as-federated-datasets","text":"During its ten years of operations, the Legacy Survey of Space and Time (LSST) will provide photometric measurements for billions of objects. Most of the foreseen LSST science cases will rely on photometric redshifts (photo-z) estimates for these objects. The LSST project plan to provide at least one, possibly more, photo-z estimates for each object as part of each data release. Given the large and diverse scope of science that can result from the LSST data, a unique photo-z method is expected to not satisfy all the requirements of the whole community. As part of the BRA-LIN in-kind contribution program, this contribution consists in offering photo-z tables as federated datasets for each data release, using a different photo-z method from the official estimates (to be defined by the DM team), thus expanding the scope of the science supported by the data releases. The infrastructure required to produce, store, and deliver the photo-z tables will be provided by the Brazilian IDAC. The software development necessary to produce these tables, which include the optimization and refactoring of the DES photo-z pipelines to run on the LSST scale and the production of new pipelines to cover all steps of the data flow, is accounted for as a directable software development effort.","title":"S4.4 - PZ Tables as Federated Datasets"},{"location":"s4_4/#software-development","text":"The initial plan of software development consisted of refactoring the pipeline Photo-z Compute from the DES Science Portal, to ensure scalability in LSST. The new pipeline would keep only the concepts of data preparation, parallelization, easy access to metadata, and provenance control. The LSST scale imposes the adoption of a completely different techlogoly from that used in the DES Portal. A preliminary version of the new pipeline Photo-z Compute is being developed using Parsl to handle the parallelization and using the photo-z code LePHARE as an example of an algorithm for tests. The code under development is available on Github . In mid-2022, the development team started an investigation to evaluate the possibility of reusing RAIL (open source, developed by DESC) infrastructure to support the production of photo-z tables. If the team decides to adopt RAIL, they would contribute to RAIL development by implementing the wrapper of the photo-z algorithm recommended by DM.","title":"Software development"},{"location":"s4_4/#planning-for-operations","text":"The figure above shows a flowchart representing the data flow within the Brazilian IDAC infrastructure. The numbered arrows refer to the sequence of processes that involves moving data through the IDAC components. The roman numbers refer to processes involving data transformation: (1) Download LSST Objects Catalog from LSST Data Access Center (DAC) to LIneA's Data Transfer Node in DMZ. (2) Move LSST Objects Catalog to Lustre Tier 1 (3) Read data ( I ) Apply data cleaning to create a \"skinny table\" to be used as input for photo-z pipelines (e.g., select columns, truncate extra decimal cases, etc) (4) Ingest skinny table into Lustre Tier 0 (fast throughput) (5) Read data ( II ) Run PZ Compute pipeline (6) Write the resulting PZ table on Lustre Tier 1 (7) Move the PZ table to LIneA's Data Transfer Node in DMZ (8) Upload the PZ table to LSST DAC as a federated dataset","title":"Planning for operations"}]}